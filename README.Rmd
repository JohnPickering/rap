---
output: github_document
author: John W Pickering
date: 12 October 2020
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# rap

<!-- badges: start -->
<!-- badges: end -->

The rap package contains functions for generating statistical metrics and visual means to assess the improvement in risk prediction of one risk model over another.  It includes the Risk Assessment Plot (hence rap).  

## Installation

You can install the development version or rap from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("JohnPickering/rap")
```

## History and versions

rap began as Matlab code in 2012 after I wrote a paper [1] for the Nephrology community on assessing the added value of one biomarker to a clinical prediction model. I worked with Professor Zoltan Endre on that paper.  Dr David Cairns kindly provided some R code for the Risk Assessment Plot.  This formed the basis of versions 0.1 to 0.4. Importantly, for those versions and the current version all errors are mine (sorry) and not those of Professor Endre or Dr Cairns.   

Version 1.02 is the current version and represents a major change and update.  First, I have used piping in the code where possible - hopefully to speed things up.  Second, I have dropped some statistical metrics that I believe are poor or wrongly applied.  In particularly, I dropped providing the total NRI (Net Reclassification Improvement) and total IDI (Integrated Discrimination Improvement) metrics.  These should never be presented because they inappropriately add together two fractions with differing denominators (NRI) or two means (IDI).  Instead, these the NRIs and IDIs for those with and without the event of interest should be provided. Third, I have provided the change in AUCs rather than a p-value because the change is much more meaningful. 

This version allows as input logistic regression models from glm() (stats) and lrm (rms) as well as risk predictions calculated elsewhere.

This version provides as outputs in addtion to the Risk Assessment Plot, a form of calibration plot and decision curve.  

Finally, the output from the main functions CI.raplot, and Ci.classNRI are lists that include the metrics for each bootstrap sample as well as the summary metrics.  Bootstrapping is used to determine confidence intervals.  

## Example 1

This is a basic example for assessing the difference between two logistic regression models:

```{r example1, warnings = FALSE, message=FALSE}
library(rap)
## basic example code

baseline_risk <- data_risk$baseline # or the glm model itself
new_risk <- data_risk$new # or the glm model itself
outcome <- data_risk$outcome

assessment <- CI.raplot(x1 = baseline_risk, x2 = new_risk, y = outcome,
                        n.boot = 20, dp = 2) # Note the default is 2000 bootstraps (n.boot = 2000).  This can take quite some time to run, so when testing I use a smaller number of bootstraps.  

# View results  
## meta data  
(assessment$meta_data)

## exact point estimates  
(assessment$Metrics)

## bootstrap derived metrics with confidence intervals  
(assessment$Summary_metrics)

```

## Graphical assessments

### The Risk Assessment Plot
```{r ggrap, warning = FALSE,  message = FALSE}
ggrap(x1 = baseline_risk, x2 = new_risk, y = outcome)

```

### The calibration curve
```{r ggcalibrate, warning = FALSE, message = FALSE}
ggcalibrate(x1 = baseline_risk, x2 = new_risk, y = outcome)

```

### The decision curve
```{r ggdecision, warning = FALSE,message = FALSE}
ggdecision(x1 = baseline_risk, x2 = new_risk, y = outcome)

```


## Example 2

This is a basic example for assessing the difference in the results of reclassification:

```{r example2, eval = FALSE,  warning = FALSE, message = FALSE}
## basic example code

baseline_class <- data_class$base_class
new_class <- data_class$new_class
outcome_class <- data_class$outcome

class_assessment <- CI.classNRI(c1 = baseline_class, c2 = new_class, y = outcome_class, n.boot = 20, dp = 2) # Note the default is 2000 bootstraps (n.boot = 2000).  This can take quite some time to run, so when testing I use a smaller number of bootstraps.  

# View results  
## meta data  
(class_assessment$meta_data)

## exact point estimates and confusion matrices
(class_assessment$Metrics)

## bootstrap derived metrics with confidence intervals  
(class_assessment$Summary_metrics)

```


